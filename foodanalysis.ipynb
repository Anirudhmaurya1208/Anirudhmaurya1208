!pip install --quiet git+https://github.com/huggingface/transformers.git
!pip install --quiet gradio
!pip install --quiet datasets
import os

#Sentiment Analysis
from transformers import pipeline
sent_classifier = pipeline("sentiment-analysis")
sent_classifier("I was pretty happy with the sneakers")

#Text Generation
generator = pipeline("text-generation")
#generator("In this course, we will teach you how to")
generator("Once upon a time,")

vision_classifier = pipeline(task="image-classification")
result = vision_classifier(
    images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
)
print("\n".join([f"Class {d['label']} with score {round(d['score'], 4)}" for d in result]))

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
#Background on the model: https://huggingface.co/facebook/nllb-200-distilled-600M
#Get list of language codes: https://github.com/facebookresearch/flores/tree/main/flores200#languages-in-flores-200

model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-200-distilled-600M")
tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-200-distilled-600M")

translator = pipeline('translation', model=model, tokenizer=tokenizer, src_lang="eng_Latn", tgt_lang='guj_Gujr')

translator("UN Chief says there is no military solution in Syria")

from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-fls")

model = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-fls")


nlp = pipeline("text-classification", model=model, tokenizer=tokenizer)
results = nlp('In the past, the age of our fleet to enhance availability and reliability due to reduced downtime for repairs.')
print(results)

# Example Snippet
import requests

API_URL = "https://api-inference.huggingface.co/models/yiyanghkust/finbert-fls"
headers = {"Authorization": "Bearer {API_TOKEN}"}   ###Add your API Key here after Bearer

def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
	"inputs": "In the past, the age of our fleet to enhance availability and reliability due to reduced downtime for repairs.",
})

from transformers import AutoFeatureExtractor, AutoModelForImageClassification
extractor = AutoFeatureExtractor.from_pretrained("rajistics/finetuned-indian-food")
model = AutoModelForImageClassification.from_pretrained("rajistics/finetuned-indian-food")

#let's get an image to test with using streaming option
from datasets import load_dataset
dataset = load_dataset("rajistics/indian_food_images",split='test', streaming=True)

imagepic = next(iter(dataset))['image']
imagepic

from PIL import Image

inputs = extractor(images=imagepic, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx])

import requests
im_url = Image.open(requests.get('https://huggingface.co/rajistics/finetuned-indian-food/resolve/main/126.jpg', stream=True).raw)
im_url

inputs = extractor(images=im_url, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx])

import gradio as gr
def greet(name):
    return "Welcome " + name + " to my lab."

demo = gr.Interface(fn=greet, inputs="text", outputs="text")
demo.launch()

import gradio as gr

gr.Interface.load("huggingface/facebook/fastspeech2-en-ljspeech").launch();

import gradio as gr
from PIL import Image

##Image Classification
from transformers import AutoFeatureExtractor, AutoModelForImageClassification
extractor = AutoFeatureExtractor.from_pretrained("rajistics/finetuned-indian-food")
model = AutoModelForImageClassification.from_pretrained("rajistics/finetuned-indian-food")

def image_to_text(imagepic):
  inputs = extractor(images=imagepic, return_tensors="pt")
  outputs = model(**inputs)
  logits = outputs.logits
  predicted_class_idx = logits.argmax(-1).item()
  return (model.config.id2label[predicted_class_idx])

##Translation
#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
#Get list of language codes: https://github.com/facebookresearch/flores/tree/main/flores200#languages-in-flores-200
#modelt = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-200-distilled-600M")
#tokenizert = AutoTokenizer.from_pretrained("facebook/nllb-200-distilled-600M")

#def translation(text):
#  translator = pipeline('translation', model=modelt, tokenizer=tokenizert, src_lang="eng_Latn", tgt_lang='ron_Latn')
#  output = translator(text)
# return (output[0]['translation_text'])

##Translation
demo = gr.Blocks()
with demo:
    image_file = gr.Image(type="pil")
    b1 = gr.Button("Recognize Image")
    text = gr.Textbox()
    b1.click(image_to_text, inputs=image_file, outputs=text)
    #b2 = gr.Button("Translation")
    #out1 = gr.Textbox()
    #b2.click(translation, inputs=text, outputs=out1)
demo.launch()
